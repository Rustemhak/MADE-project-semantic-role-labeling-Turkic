{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-12-20T06:13:23.597394Z",
     "iopub.status.busy": "2022-12-20T06:13:23.596820Z",
     "iopub.status.idle": "2022-12-20T06:13:36.519721Z",
     "shell.execute_reply": "2022-12-20T06:13:36.517669Z",
     "shell.execute_reply.started": "2022-12-20T06:13:23.597344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bertviz allennlp-models -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T06:04:04.178629Z",
     "iopub.status.busy": "2022-12-20T06:04:04.177985Z",
     "iopub.status.idle": "2022-12-20T06:04:18.100325Z",
     "shell.execute_reply": "2022-12-20T06:04:18.098724Z",
     "shell.execute_reply.started": "2022-12-20T06:04:04.178575Z"
    }
   },
   "outputs": [],
   "source": [
    "import allennlp_models.tagging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests, json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from bertviz import head_view, model_view\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T09:53:05.308147Z",
     "iopub.status.busy": "2022-12-20T09:53:05.307630Z",
     "iopub.status.idle": "2022-12-20T09:53:05.349729Z",
     "shell.execute_reply": "2022-12-20T09:53:05.348542Z",
     "shell.execute_reply.started": "2022-12-20T09:53:05.308111Z"
    }
   },
   "outputs": [],
   "source": [
    "class Model_srl:\n",
    "    def __init__(self, path_to_model = \"../input/xlmrobertamultilang/xlm-roberta\"):\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(path_to_model, num_labels=2)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "        self.sep_token = \" </s> \"\n",
    "        self.srl_model = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n",
    "        \n",
    "        self.att_threshold = 0.1e-00\n",
    "        self.att_n_tok_per_word = 1\n",
    "        self.att_layer = 6\n",
    "        self.att_head = 2\n",
    "#         self.ENG_SENTENCE = ENG_SENTENCE\n",
    "        self.IAM_TOKEN = 't1.9euelZqSiZPKmM-OyZ2MzceemMvNyO3rnpWam46Pjp6Qj5uam5KQxo_Pxo3l9PdOCXti-e9KNAz63fT3Djh4YvnvSjQM-g.a2bezOMde36Yt9BB9m-1CirUQ2tdrtiNPPTuVsrbt7fxNhR1vixnCENNb_wso5wTWAzcIoX8NQd78Tc5BtcACw'\n",
    "        self.folder_id = 'b1grvnc3e8sgtm7qcsja'\n",
    "        \n",
    "    def translate(self, texts, source_language='tt', target_language='en'):\n",
    "        body = {\n",
    "            \"targetLanguageCode\": target_language,\n",
    "            \"texts\": texts,\n",
    "            \"folderId\": self.folder_id,\n",
    "            \"sourceLanguageCode\":  source_language\n",
    "        }\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": \"Bearer {0}\".format(self.IAM_TOKEN)\n",
    "        }\n",
    "        url = 'https://translate.api.cloud.yandex.net/translate/v2/translate'\n",
    "        response = requests.post(url,\n",
    "            json = body,\n",
    "            headers = headers\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            print('Ожидаю 0.5 секунды...')  \n",
    "            time.sleep(0.5)\n",
    "            response = requests.post(url,\n",
    "            json = body,\n",
    "            headers = headers\n",
    "            )\n",
    "        d = json.loads(response.text)\n",
    "        translations = d['translations']\n",
    "        return [t['text'] for t in translations]\n",
    "        \n",
    "    def get_vocab(self, sentence):\n",
    "        self.tokenized = self.tokenizer([sentence], return_tensors='pt').to(self.model.device)\n",
    "        self.tokens = self.tokenizer.convert_ids_to_tokens(self.tokenized['input_ids'][0].tolist())\n",
    "        self.tok_to_idx = {token: idx for idx, token in enumerate(self.tokens)}\n",
    "        self.idx_to_tok = {idx: token  for idx, token in enumerate(self.tokens)}\n",
    "    \n",
    "    def get_word_to_tok(self, desired_output, sentence):\n",
    "        word2tok_dict = {x:desired_output[i] for i, x in enumerate(sentence.split())}\n",
    "        self.word2tok = pd.DataFrame.from_dict({'word': word2tok_dict.keys() , 'tokens': word2tok_dict.values()})\n",
    "        self.get_vocab(sentence)\n",
    "        \n",
    "    def get_desired_output(self, sentence):\n",
    "        idx = 1\n",
    "        enc =[self.tokenizer.encode(x, add_special_tokens=False) for x in sentence.split()]\n",
    "\n",
    "        desired_output = []\n",
    "\n",
    "        for token in enc:\n",
    "            tokenoutput = []\n",
    "            for ids in token:\n",
    "                tokenoutput.append(idx)\n",
    "                idx +=1\n",
    "            desired_output.append(tokenoutput)\n",
    "        \n",
    "        self.get_word_to_tok(desired_output, sentence)\n",
    "            \n",
    "        return desired_output\n",
    "    \n",
    "    def get_words_attention(self, attention):\n",
    "        N = self.word2tok.shape[0]\n",
    "        d = np.array([[0.] * len(self.tokens) for _ in range(N)])\n",
    "        p = np.array([[0.] * N for _ in range(N)])\n",
    "        for idx, token in enumerate(self.tokens):\n",
    "            for i, word_tok in enumerate(self.word2tok.tokens.values):\n",
    "                if idx in word_tok and d[i].sum() == 0:\n",
    "                    d[i] = attention[self.word2tok.loc[i,'tokens']].detach().numpy().sum(axis=0)\n",
    "\n",
    "        for idx, token in enumerate(self.tokens):\n",
    "            for i, word_tok in enumerate(self.word2tok.tokens.values):\n",
    "                if idx in word_tok and p[:,i].sum() == 0:\n",
    "                    p[:,i] = d[:,self.word2tok.loc[i,'tokens']].sum(axis=1)\n",
    "        return p\n",
    "\n",
    "    def get_mapping_dict(self, p):\n",
    "        N = self.word2tok.shape[0]\n",
    "        mapping_dict = {}\n",
    "        for word_idx in range(N):\n",
    "            if self.word2tok.loc[word_idx, 'word'] == \"</s>\":\n",
    "                break\n",
    "            mask = (p[word_idx] >= self.att_threshold)\n",
    "            attention_word = p[word_idx][mask]\n",
    "            map_word = list(self.word2tok.word.values[mask])\n",
    "            # Проверяем наличие исходного слова, если есть удаляем\n",
    "            clean_map_word = []\n",
    "            clean_attention_word = []\n",
    "            for w in map_word:\n",
    "                i = map_word.index(w)\n",
    "                if w not in [self.word2tok.loc[word_idx, 'word'], \"</s>\"]:\n",
    "                    clean_map_word.append(w)\n",
    "            clean_map_word = [self.drop_punc(w) for w in clean_map_word]\n",
    "            if len(clean_map_word) != 0:\n",
    "                mapping_dict[self.word2tok.loc[word_idx, 'word']] = clean_map_word\n",
    "\n",
    "        return mapping_dict\n",
    "                \n",
    "    \n",
    "    def drop_punc(self, word):\n",
    "        punc = '''!()-[]{};:\"\\,<>./?@#$%^&*_~'''\n",
    "        for p in punc:\n",
    "            word = word.replace(p,'')\n",
    "        return word\n",
    "\n",
    "    def make_dict(self, description):\n",
    "        res = {}\n",
    "        for i,char in enumerate(description):\n",
    "            if char == '[':\n",
    "                begin = i + 1\n",
    "            if char == ']':\n",
    "                finish = i\n",
    "                role = description[begin:finish].split(': ')\n",
    "                res[role[0]] = role[1]\n",
    "        return res\n",
    "\n",
    "    def mapping(self, mapping_dict, eng_words):\n",
    "        res = ''\n",
    "        eng_words = eng_words.split()\n",
    "        for  i, word in enumerate(eng_words):\n",
    "            try:\n",
    "                for j, w in enumerate(mapping_dict[word]):\n",
    "                    if w not in res:\n",
    "                        res += ' '\n",
    "                        res += w\n",
    "            except:\n",
    "                continue\n",
    "        return res[1:]\n",
    "\n",
    "    def result(self, mapping_dict, roles, tat_sentence):\n",
    "        new_roles = []\n",
    "        for verb in roles['verbs']:\n",
    "            result = {}\n",
    "            srl_verb = {}  \n",
    "\n",
    "            # Делаем маппинг ролей в виде словаря\n",
    "            description = verb['description']\n",
    "            description = self.make_dict(description)\n",
    "            for srl in description:\n",
    "                mapped_word = self.mapping(mapping_dict, description[srl])\n",
    "                srl_verb[srl] = mapped_word\n",
    "            keys = srl_verb.keys()\n",
    "            \n",
    "            # Если нет разметки после мапинга, то пропускаем этот глагол\n",
    "            if len(keys) == 1 and 'V' in keys:\n",
    "                continue\n",
    "            \n",
    "            # Сохраняем глагол предикат\n",
    "            new_verb = self.mapping(mapping_dict, verb['verb'])\n",
    "            # Записывваем тэги на основании словаря ролей\n",
    "            sentence = tat_sentence.split()\n",
    "            new_tags = []\n",
    "\n",
    "            for word in sentence:\n",
    "                for srl in keys:  \n",
    "                    if self.drop_punc(word) in srl_verb[srl]:\n",
    "                        role = srl\n",
    "                        break\n",
    "                    else:\n",
    "                        role = 'O'\n",
    "                new_tags.append(role)\n",
    "\n",
    "            result['verb'] = new_verb\n",
    "            result['description'] = srl_verb\n",
    "            result['tags'] = new_tags\n",
    "\n",
    "            new_roles.append(result)\n",
    "        return new_roles\n",
    "    \n",
    "\n",
    "    def predict(self, tat_sentence, source_language):\n",
    "        eng_sentence = self.translate(tat_sentence, source_language)[0]\n",
    "        roles = self.srl_model.predict(sentence=eng_sentence)\n",
    "        \n",
    "        sentence = eng_sentence + self.sep_token + tat_sentence\n",
    "        \n",
    "        desired_output = self.get_desired_output(sentence)\n",
    "        \n",
    "        attention = self.model(\n",
    "            input_ids=self.tokenized.input_ids,\n",
    "            attention_mask=self.tokenized.attention_mask,\n",
    "            output_attentions = True\n",
    "        )['attentions'][self.att_layer][0][self.att_head]\n",
    "        \n",
    "        att = self.get_words_attention(attention)\n",
    "        mapping_dict = self.get_mapping_dict(att)\n",
    "        new_roles = {}\n",
    "        new_roles['verbs'] = self.result(mapping_dict, roles, tat_sentence)\n",
    "        new_roles['words'] = tat_sentence.split()\n",
    "        return new_roles\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T09:53:06.460183Z",
     "iopub.status.busy": "2022-12-20T09:53:06.459697Z",
     "iopub.status.idle": "2022-12-20T09:53:40.648922Z",
     "shell.execute_reply": "2022-12-20T09:53:40.647727Z",
     "shell.execute_reply.started": "2022-12-20T09:53:06.460134Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Model_srl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T06:27:44.226882Z",
     "iopub.status.busy": "2022-12-20T06:27:44.225013Z",
     "iopub.status.idle": "2022-12-20T06:27:44.233951Z",
     "shell.execute_reply": "2022-12-20T06:27:44.232279Z",
     "shell.execute_reply.started": "2022-12-20T06:27:44.226792Z"
    }
   },
   "outputs": [],
   "source": [
    "TAT_SENTENCE = 'Бинага керү өчен кирәк булган ачкычлар машинада бикләнгән иде.'\n",
    "KZ_SENTENCE = 'Ғимаратқа кіру үшін қажетті кілттер көлікте құлыптаулы болған.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T06:27:57.886304Z",
     "iopub.status.busy": "2022-12-20T06:27:57.885756Z",
     "iopub.status.idle": "2022-12-20T06:28:01.349249Z",
     "shell.execute_reply": "2022-12-20T06:28:01.347337Z",
     "shell.execute_reply.started": "2022-12-20T06:27:57.886253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TAT \n",
      " {'verbs': [{'verb': 'кирәк', 'description': {'ARG1': 'ачкычлар', 'V': 'кирәк', 'ARGM-PRP': 'керү өчен машинада'}, 'tags': ['O', 'ARGM-PRP', 'ARGM-PRP', 'V', 'O', 'ARG1', 'ARGM-PRP', 'O', 'O']}, {'verb': 'керү', 'description': {'V': 'керү', 'ARG1': 'машинада'}, 'tags': ['O', 'V', 'O', 'O', 'O', 'O', 'ARG1', 'O', 'O']}, {'verb': 'ачкычлар', 'description': {'ARG1': 'ачкычлар кирәк керү өчен машинада', 'V': 'ачкычлар', 'ARGM-LOC': 'Бинага машинада'}, 'tags': ['ARGM-LOC', 'ARG1', 'ARG1', 'ARG1', 'O', 'ARG1', 'ARG1', 'O', 'O']}], 'words': ['Бинага', 'керү', 'өчен', 'кирәк', 'булган', 'ачкычлар', 'машинада', 'бикләнгән', 'иде.']}\n",
      "\n",
      " KZ \n",
      " {'verbs': [{'verb': 'қажетті', 'description': {'ARG1': 'қажетті кілттер', 'V': 'қажетті', 'ARGM-PRP': 'үшін кіру көлікте'}, 'tags': ['O', 'ARGM-PRP', 'ARGM-PRP', 'ARG1', 'ARG1', 'ARGM-PRP', 'O', 'O']}, {'verb': 'кіру', 'description': {'V': 'кіру', 'ARG1': 'көлікте'}, 'tags': ['O', 'V', 'O', 'O', 'O', 'ARG1', 'O', 'O']}, {'verb': 'құлыптаулы', 'description': {'ARG1': 'қажетті кілттер үшін кіру көлікте', 'V': 'құлыптаулы', 'ARGM-LOC': 'көлікте'}, 'tags': ['O', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'ARG1', 'V', 'O']}], 'words': ['Ғимаратқа', 'кіру', 'үшін', 'қажетті', 'кілттер', 'көлікте', 'құлыптаулы', 'болған.']}\n",
      "CPU times: user 1.35 s, sys: 5.91 ms, total: 1.36 s\n",
      "Wall time: 3.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('\\n TAT \\n', model.predict(TAT_SENTENCE, 'tt'))\n",
    "print('\\n KZ \\n', model.predict(KZ_SENTENCE, 'kk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T06:28:21.042049Z",
     "iopub.status.busy": "2022-12-20T06:28:21.041500Z",
     "iopub.status.idle": "2022-12-20T06:28:21.048790Z",
     "shell.execute_reply": "2022-12-20T06:28:21.046947Z",
     "shell.execute_reply.started": "2022-12-20T06:28:21.042006Z"
    }
   },
   "outputs": [],
   "source": [
    "TAT_SENTENCE = 'Бүген көн буранлы, әмма салкын түгел'\n",
    "KZ_SENTENCE = 'Бүгін боран, бірақ суық емес'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T06:28:21.232702Z",
     "iopub.status.busy": "2022-12-20T06:28:21.232182Z",
     "iopub.status.idle": "2022-12-20T06:28:24.410478Z",
     "shell.execute_reply": "2022-12-20T06:28:24.409158Z",
     "shell.execute_reply.started": "2022-12-20T06:28:21.232656Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TAT \n",
      " {'verbs': [{'verb': '', 'description': {'ARG1': '', 'V': '', 'ARG2': 'көн', 'ARGM-TMP': ''}, 'tags': ['O', 'ARG2', 'O', 'O', 'O', 'O']}, {'verb': '', 'description': {'ARG1': '', 'V': '', 'ARGM-NEG': 'түгел', 'ARG2': ''}, 'tags': ['O', 'O', 'O', 'O', 'O', 'ARGM-NEG']}], 'words': ['Бүген', 'көн', 'буранлы,', 'әмма', 'салкын', 'түгел']}\n",
      "\n",
      " KZ \n",
      " {'verbs': [{'verb': '', 'description': {'ARG1': 'Бүгін', 'V': '', 'ARG2': 'бірақ емес суық'}, 'tags': ['ARG1', 'O', 'ARG2', 'ARG2', 'ARG2']}], 'words': ['Бүгін', 'боран,', 'бірақ', 'суық', 'емес']}\n",
      "CPU times: user 948 ms, sys: 6.81 ms, total: 955 ms\n",
      "Wall time: 3.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('\\n TAT \\n', model.predict(TAT_SENTENCE, 'tt'))\n",
    "print('\\n KZ \\n', model.predict(KZ_SENTENCE, 'kk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T06:28:24.413300Z",
     "iopub.status.busy": "2022-12-20T06:28:24.412901Z",
     "iopub.status.idle": "2022-12-20T06:28:24.419291Z",
     "shell.execute_reply": "2022-12-20T06:28:24.417692Z",
     "shell.execute_reply.started": "2022-12-20T06:28:24.413259Z"
    }
   },
   "outputs": [],
   "source": [
    "TAT_SENTENCE = 'Без дачага киттек'\n",
    "KZ_SENTENCE = 'Біз коттеджге бардық'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-20T06:28:24.421881Z",
     "iopub.status.busy": "2022-12-20T06:28:24.421390Z",
     "iopub.status.idle": "2022-12-20T06:28:27.363203Z",
     "shell.execute_reply": "2022-12-20T06:28:27.361568Z",
     "shell.execute_reply.started": "2022-12-20T06:28:24.421831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TAT \n",
      " {'verbs': [{'verb': 'киттек', 'description': {'ARG0': 'Без', 'V': 'киттек', 'ARG4': 'дачага'}, 'tags': ['ARG0', 'ARG4', 'V']}], 'words': ['Без', 'дачага', 'киттек']}\n",
      "\n",
      " KZ \n",
      " {'verbs': [{'verb': 'бардық', 'description': {'ARG0': 'Біз', 'V': 'бардық', 'ARG4': 'бардық коттеджге'}, 'tags': ['ARG0', 'ARG4', 'V']}], 'words': ['Біз', 'коттеджге', 'бардық']}\n",
      "CPU times: user 753 ms, sys: 5.03 ms, total: 758 ms\n",
      "Wall time: 2.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('\\n TAT \\n', model.predict(TAT_SENTENCE, 'tt'))\n",
    "print('\\n KZ \\n', model.predict(KZ_SENTENCE, 'kk'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
