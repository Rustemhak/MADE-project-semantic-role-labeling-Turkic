{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53752a4f-6d99-4760-a8db-26ff41e66928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7e7ea-a9bf-40e6-9b5e-8dae20e5a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14e0ede6-e860-4564-8992-afcda163853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf7af7ec-65a5-4b5e-9f0b-2bc7804136c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "from pprint import pprint as print_\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from isanlp_srl_framebank.pipeline_default import PipelineDefault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dce71b-7cb5-479b-b824-5e317ba4eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d877392d-f428-4464-9392-b03596efff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100   341  100   252  100    89   1296    457 --:--:-- --:--:-- --:--:--  1766\n"
     ]
    }
   ],
   "source": [
    "!curl -d \"{\\\"yandexPassportOauthToken\\\":\\\"y0_AgAAAAAiUyyhAATuwQAAAADS3TX_f0yBTjPTRMCBygtxvEC5suQjajg\\\"}\" \"https://iam.api.cloud.yandex.net/iam/v1/tokens\" -o iamToken.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ab5b8cd-836f-4a51-a1f6-970fbc58364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('iamToken.txt') as f:\n",
    "    d = json.loads(f.read())\n",
    "    IAM_TOKEN = d['iamToken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bc4d435d-dd7e-4551-bdeb-9d3922761eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t1.9euelZqZyZGXxpWSkJudjYqWzJSKnO3rnpWaj46Nkc6ejZaLyZvOmZqWj53l9Pc6I1Rj-e9RRU383fT3elFRY_nvUUVN_A.WPo9dx1BCSQpy4_p66KPc0ZZiU-i4jHc0pruaZhoRb1CUbvs56qg5TvmTOP2wsZzBV8ZPddjWRHnV1Q6keLDBA'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IAM_TOKEN = 't1.9euelZqaz5DGjI6Py5bLnoqTlo_Oju3rnpWaj46Nkc6ejZaLyZvOmZqWj53l8_dIF2Zk-e9eDic9_t3z9whGY2T5714OJz3-.1UJE5innTD10cDR2-7YbCzvwem5VKOvoIQCksQwLbJAA7oAdm4OpX9lgQyC6cosbkLvAMqxKaFT1SfQlDXTlBg'\n",
    "folder_id = 'b1gs2kplab3ve997i80t'\n",
    "IAM_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "246b6871-00c1-445b-8012-6af1d9139e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0645e314-9eb8-48f3-aea0-e39668c6c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run --rm -p 3333:3333 inemo/isanlp\n",
    "# docker run --rm --shm-size=1024m -ti -p 3334:9999 inemo/syntaxnet_rus server 0.0.0.0 9999\n",
    "# docker run --rm -p 3335:3333 inemo/isanlp_srl_framebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a25271b1-4aa5-427a-b82f-ea30a173cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp_srl_framebank.pipeline_default import PipelineDefault  \n",
    "#\n",
    "ppl = PipelineDefault(address_morph=('localhost', 3333),\n",
    "                      address_syntax=('localhost', 3334),\n",
    "                      address_srl=('localhost', 3335))\n",
    "res = ppl('Мы поехали на дачу.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f94b6ca7-3646-4b30-87e8-b80c441494ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 4.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "language = languages[0]\n",
    "url = 'https://beta.apertium.org/index.eng.html#analysis?aLang=' + language + '&aQ='    \n",
    "browser = webdriver.Chrome()\n",
    "browser.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a13a547e-d60b-4300-b1f5-939db94c12c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "10d2ab99-a093-4e19-939d-37b33b2d8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_srl1:\n",
    "    def __init__(self, RUS_SENTENCE=None):\n",
    "        self.RUS_SENTENCE = RUS_SENTENCE\n",
    "        self.ppl = PipelineDefault(address_morph=('localhost', 3333),\n",
    "                      address_syntax=('localhost', 3334),\n",
    "                      address_srl=('localhost', 3335))\n",
    "        self.language_code_dict =  {\n",
    "        'татарский': 'tt',\n",
    "        'казахский' : 'kk',\n",
    "        'башкирский':'ba',\n",
    "        'киргизский' : 'ky'\n",
    "        }\n",
    "\n",
    "    def translation(self, texts, source_language='tt', target_language='ru'):\n",
    "        #print('texts', texts)\n",
    "        body = {\n",
    "            \"targetLanguageCode\": target_language,\n",
    "            \"texts\": texts,\n",
    "            \"folderId\": folder_id,\n",
    "            \"sourceLanguageCode\":  source_language\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": \"Bearer {0}\".format(IAM_TOKEN)\n",
    "        }\n",
    "        url = 'https://translate.api.cloud.yandex.net/translate/v2/translate'\n",
    "        response = requests.post(url,\n",
    "            json = body,\n",
    "            headers = headers\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            print('Ожидаю 0.5 секунды...')  \n",
    "            time.sleep(0.5)\n",
    "            response = requests.post(url,\n",
    "            json = body,\n",
    "            headers = headers\n",
    "            )\n",
    "        d = json.loads(response.text)\n",
    "        #print(texts, d)\n",
    "        translations = d['translations']\n",
    "        return [t['text'] for t in translations]\n",
    "    \n",
    "    def drop_punkt(self, word):\n",
    "        punc = '''!()-[]{};:\"\\,<>./?@#$%^&*_~'''\n",
    "        for p in punc:\n",
    "            word = word.replace(p,'')\n",
    "        return word\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        words = text.split()\n",
    "        return [self.drop_punkt(word) for word in words]\n",
    "    \n",
    "    def get_morph(self, text, language):\n",
    "        lang_field = browser.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/form/div[1]/div/select')\n",
    "        lang_field.send_keys(language)\n",
    "\n",
    "        text_field = browser.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/form/div[2]/div/textarea').clear()\n",
    "        text_field = browser.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/form/div[2]/div/textarea')\n",
    "\n",
    "        text = drop_punc(text)\n",
    "        text_field.send_keys(text)\n",
    "\n",
    "        button = browser.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/form/div[3]/div/button')\n",
    "        button.click()\n",
    "        sleep(1)\n",
    "\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        translation = soup.find_all('td',class_=\"text-left\")\n",
    "        words = []\n",
    "        for word in translation:\n",
    "            word = word.text.split()\n",
    "            for i, w in enumerate(word):\n",
    "                if w == '↤':\n",
    "                    break\n",
    "            word = word[:i]\n",
    "            for w in word:\n",
    "                words.append(w)\n",
    "        return words\n",
    "\n",
    "    \n",
    "    # def get_morph_list(self, words):\n",
    "    #     result = []\n",
    "    #     for i, word in enumerate(words):\n",
    "    #         result.append(get_morph(word))\n",
    "    #     return result\n",
    "    \n",
    "    def print_roles(self, lemma, role_annot, targetLanguage=None):\n",
    "        word_role = {}\n",
    "        for sent_num, ann_sent in enumerate(role_annot):\n",
    "            for event in ann_sent:\n",
    "                lemma_pred = lemma[sent_num][event.pred[0]]\n",
    "                if targetLanguage is not None:\n",
    "                    trg_lang = self.language_code_dict[targetLanguage]\n",
    "                    src_lang = 'ru'\n",
    "                    #print('=====Pred: {}'.format(self.translation(lemma_pred, src_lang, trg_lang)[0]))\n",
    "                    word_role[self.translation(lemma_pred, src_lang, trg_lang)[0]] = 'Pred'\n",
    "                else:\n",
    "                    print('=====Pred: {}'.format(lemma_pred))\n",
    "                for arg in event.args:\n",
    "                    lemma_arg  = lemma[sent_num][arg.begin]\n",
    "                    if targetLanguage is not None:\n",
    "                        #print('Arg({}): {}'.format(arg.tag, self.translation(lemma_arg, src_lang, trg_lang)[0]))\n",
    "                        word_role[self.translation(lemma_arg, src_lang, trg_lang)[0]] = 'Arg({})'.format(arg.tag)\n",
    "                    else:\n",
    "                        print('Arg({}): {}'.format(arg.tag, lemma_arg))\n",
    "        return word_role\n",
    "    \n",
    "    def get_srl_turkic(self, rus_text, selected_language):\n",
    "        # texts = [\"Без дачага киттек.\", \"Балалар өйдән кача һәм урманда югалып кала.\",\"Әни рамны юды. Әти машинаны йөртте.\"]\n",
    "        # словарь - лемма:роль\n",
    "        # src_code = self.language_code_dict[selected_language]\n",
    "        # translation = translate(text, source_language=src_code )\n",
    "        # # print(translations)\n",
    "        # print(f'исходный  текст на {selected_language[:-2]}ом', end='\\n\\n')\n",
    "        # rus_text = translation\n",
    "        # print('переведенный текст на русском', end='\\n\\n')\n",
    "        # print(rus_text)\n",
    "        #print('get_srl_turkic_rus_text', rus_text)\n",
    "        res = ppl(rus_text)\n",
    "        # print(f'разметка семантических ролей на русском')\n",
    "        # print_roles(res['lemma'], res['srl'])\n",
    "        # print()\n",
    "        # print(f'разметка семантических ролей на {selected_language[:-2]}ом')\n",
    "        return self.print_roles(res['lemma'], res['srl'], selected_language)\n",
    "        \n",
    "        \n",
    "    def predict(self, input_sentences, language_ru_name):\n",
    "        \n",
    "        languages_rus__eng = {'татарский':'Tatar', 'казахский':'Kazakh', 'киргизский':'Kyrgyz', 'башкирский':'Bashqort'}\n",
    "        for input_sentence in input_sentences:\n",
    "            #перевод\n",
    "            rus_sentence = self.translation(input_sentence, source_language=self.language_code_dict[language_ru_name])[0]\n",
    "            # токенизация исходного предложения\n",
    "            #input_words = tokenize(input_sentence)\n",
    "            # стемминг слов исходного предложения\n",
    "            stem_input_words = self.get_morph(input_sentence, languages_rus__eng[language_ru_name])\n",
    "            #print('stems',stem_input_words)\n",
    "            # получить список словарей лемма - роль\n",
    "            word_roles = self.get_srl_turkic(rus_sentence, language_ru_name)\n",
    "            #print('word roles',word_roles)\n",
    "            # применить к леммам стемминг\n",
    "            stem_role = {self.get_morph(k, languages_rus__eng[language_ru_name])[0]: v for k, v in word_roles.items()}\n",
    "            #print('stem role', stem_role)\n",
    "            # разметить всю последовательность по словарям полученным выше\n",
    "            result = {}\n",
    "            for stem_input_word in stem_input_words:\n",
    "                if stem_input_word in stem_role:\n",
    "                    result[stem_input_word] = stem_role[stem_input_word]\n",
    "                else:\n",
    "                    result[stem_input_word] = 'O'\n",
    "            result_input = {}\n",
    "            for k, v in zip(self.tokenize(input_sentence), result.values()):\n",
    "                result_input[k] = v\n",
    "            print(result_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ed7fe3d8-aa5e-4b89-96ab-c5b9b1ce1ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl = Model_srl1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c017940a-d393-4ef7-bd84-ffa6bfd6c6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Петр': 'O', 'дачага': 'Arg(конечная точка)', 'барыр': 'Pred', 'өчен': 'O', 'машина': 'Arg(пациенс)', 'сатып': 'O', 'алган': 'Pred'}\n",
      "{'Петр': 'O', 'алманы': 'Arg(пациенс)', 'Иваннан': 'O', '5': 'O', 'сумга': 'O', 'сатып': 'O', 'алган': 'Pred'}\n",
      "{'Без': 'Arg(субъект перемещения)', 'дачага': 'Arg(конечная точка)', 'киттек': 'O'}\n",
      "{'Балалар': 'Arg(субъект перемещения)', 'өйдән': 'O', 'кача': 'Pred', 'һәм': 'O', 'урманда': 'Arg(место)', 'югалып': 'O', 'кала': 'O'}\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('text_tt.txt', encoding='utf-8') as f:\n",
    "    texts_tt = f.readlines()\n",
    "selected_language = 'татарский'\n",
    "result = srl.predict(texts_tt, selected_language)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "made-py39-venv",
   "language": "python",
   "name": "made-py39-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
