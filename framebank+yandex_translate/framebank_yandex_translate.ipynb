{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53752a4f-6d99-4760-a8db-26ff41e66928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c7e7ea-a9bf-40e6-9b5e-8dae20e5a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e0ede6-e860-4564-8992-afcda163853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7af7ec-65a5-4b5e-9f0b-2bc7804136c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "from pprint import pprint as print_\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from isanlp_srl_framebank.pipeline_default import PipelineDefault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d877392d-f428-4464-9392-b03596efff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:02 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:03 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:04 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:05 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:06 --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:--  0:00:07 --:--:--     0\n",
      "100    89    0     0  100    89      0     10  0:00:08  0:00:08 --:--:--    16\n",
      "100    89    0     0  100    89      0      9  0:00:09  0:00:09 --:--:--    16\n",
      "100    89    0     0  100    89      0      8  0:00:11  0:00:10  0:00:01    16\n",
      "100    89    0     0  100    89      0      7  0:00:12  0:00:11  0:00:01    16\n",
      "100    89    0     0  100    89      0      7  0:00:12  0:00:12 --:--:--    16\n",
      "100    89    0     0  100    89      0      6  0:00:14  0:00:13  0:00:01     0\n",
      "100    89    0     0  100    89      0      6  0:00:14  0:00:14 --:--:--     0\n",
      "100    89    0     0  100    89      0      5  0:00:17  0:00:15  0:00:02     0\n",
      "100   341  100   252  100    89     16      5  0:00:17  0:00:15  0:00:02    69\n"
     ]
    }
   ],
   "source": [
    "!curl -d \"{\\\"yandexPassportOauthToken\\\":\\\"y0_AgAAAAAiUyyhAATuwQAAAADS3TX_f0yBTjPTRMCBygtxvEC5suQjajg\\\"}\" \"https://iam.api.cloud.yandex.net/iam/v1/tokens\" -o iamToken.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab5b8cd-836f-4a51-a1f6-970fbc58364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('iamToken.txt') as f:\n",
    "    d = json.loads(f.read())\n",
    "    IAM_TOKEN = d['iamToken']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc4d435d-dd7e-4551-bdeb-9d3922761eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t1.9euelZrHyMjNk52Xz5eMypmTko6Onu3rnpWaj46Nkc6ejZaLyZvOmZqWj53l9PdTC1Rj-e9QczDf3fT3EzpRY_nvUHMw3w.PLSvD3eY6JfXmXgYbFhNEyQlEKDf0Vr2kG0vi2-2TAj157wGCqTtJ7IIpXTuNaR6WxducZbOp5DgPY7WRl1GAA'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IAM_TOKEN = 't1.9euelZqaz5DGjI6Py5bLnoqTlo_Oju3rnpWaj46Nkc6ejZaLyZvOmZqWj53l8_dIF2Zk-e9eDic9_t3z9whGY2T5714OJz3-.1UJE5innTD10cDR2-7YbCzvwem5VKOvoIQCksQwLbJAA7oAdm4OpX9lgQyC6cosbkLvAMqxKaFT1SfQlDXTlBg'\n",
    "folder_id = 'b1gs2kplab3ve997i80t'\n",
    "IAM_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b6871-00c1-445b-8012-6af1d9139e51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0645e314-9eb8-48f3-aea0-e39668c6c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run --rm -p 3333:3333 inemo/isanlp\n",
    "# docker run --rm --shm-size=1024m -ti -p 3334:9999 inemo/syntaxnet_rus server 0.0.0.0 9999\n",
    "# docker run --rm -p 3335:3333 inemo/isanlp_srl_framebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a25271b1-4aa5-427a-b82f-ea30a173cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp_srl_framebank.pipeline_default import PipelineDefault  \n",
    "#\n",
    "ppl = PipelineDefault(address_morph=('localhost', 3333),\n",
    "                      address_syntax=('localhost', 3334),\n",
    "                      address_srl=('localhost', 3335))\n",
    "res = ppl('Мы поехали на дачу.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f94b6ca7-3646-4b30-87e8-b80c441494ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'languages' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'languages' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "language = languages[0]\n",
    "url = 'https://beta.apertium.org/index.eng.html#analysis?aLang=' + language + '&aQ='    \n",
    "browser = webdriver.Chrome()\n",
    "browser.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13a547e-d60b-4300-b1f5-939db94c12c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10d2ab99-a093-4e19-939d-37b33b2d8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_srl1:\n",
    "    def __init__(self, RUS_SENTENCE=None):\n",
    "        self.RUS_SENTENCE = RUS_SENTENCE\n",
    "        self.ppl = PipelineDefault(address_morph=('localhost', 3333),\n",
    "                      address_syntax=('localhost', 3334),\n",
    "                      address_srl=('localhost', 3335))\n",
    "        self.language_code_dict =  {\n",
    "        'татарский': 'tt',\n",
    "        'казахский' : 'kk',\n",
    "        'башкирский':'ba',\n",
    "        'киргизский' : 'ky'\n",
    "        }\n",
    "\n",
    "    def translation(self, texts, source_language='tt', target_language='ru'):\n",
    "        #print('texts', texts)\n",
    "        body = {\n",
    "            \"targetLanguageCode\": target_language,\n",
    "            \"texts\": texts,\n",
    "            \"folderId\": folder_id,\n",
    "            \"sourceLanguageCode\":  source_language\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": \"Bearer {0}\".format(IAM_TOKEN)\n",
    "        }\n",
    "        url = 'https://translate.api.cloud.yandex.net/translate/v2/translate'\n",
    "        response = requests.post(url,\n",
    "            json = body,\n",
    "            headers = headers\n",
    "        )\n",
    "        if response.status_code != 200:\n",
    "            print('Ожидаю 0.5 секунды...')  \n",
    "            time.sleep(0.5)\n",
    "            response = requests.post(url,\n",
    "            json = body,\n",
    "            headers = headers\n",
    "            )\n",
    "        d = json.loads(response.text)\n",
    "        #print(texts, d)\n",
    "        translations = d['translations']\n",
    "        return [t['text'] for t in translations]\n",
    "    \n",
    "    def drop_punkt(self, word):\n",
    "        punc = '''!()-[]{};:\"\\,<>./?@#$%^&*_~'''\n",
    "        for p in punc:\n",
    "            word = word.replace(p,'')\n",
    "        return word\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        words = text.split()\n",
    "        return [self.drop_punkt(word) for word in words]\n",
    "    \n",
    "    def get_morph(self, text, language):\n",
    "        lang_field = browser.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/form/div[1]/div/select')\n",
    "        lang_field.send_keys(language)\n",
    "\n",
    "        text_field = browser.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/form/div[2]/div/textarea').clear()\n",
    "        text_field = browser.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/form/div[2]/div/textarea')\n",
    "\n",
    "        text = drop_punc(text)\n",
    "        text_field.send_keys(text)\n",
    "\n",
    "        button = browser.find_element(By.XPATH, '/html/body/div/div[1]/div[1]/form/div[3]/div/button')\n",
    "        button.click()\n",
    "        sleep(1)\n",
    "\n",
    "        html = browser.page_source\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        translation = soup.find_all('td',class_=\"text-left\")\n",
    "        words = []\n",
    "        for word in translation:\n",
    "            word = word.text.split()\n",
    "            for i, w in enumerate(word):\n",
    "                if w == '↤':\n",
    "                    break\n",
    "            word = word[:i]\n",
    "            for w in word:\n",
    "                words.append(w)\n",
    "        return words\n",
    "\n",
    "    \n",
    "    # def get_morph_list(self, words):\n",
    "    #     result = []\n",
    "    #     for i, word in enumerate(words):\n",
    "    #         result.append(get_morph(word))\n",
    "    #     return result\n",
    "    \n",
    "    def print_roles(self, lemma, role_annot, targetLanguage=None):\n",
    "        word_role = {}\n",
    "        for sent_num, ann_sent in enumerate(role_annot):\n",
    "            for event in ann_sent:\n",
    "                lemma_pred = lemma[sent_num][event.pred[0]]\n",
    "                if targetLanguage is not None:\n",
    "                    trg_lang = self.language_code_dict[targetLanguage]\n",
    "                    src_lang = 'ru'\n",
    "                    #print('=====Pred: {}'.format(self.translation(lemma_pred, src_lang, trg_lang)[0]))\n",
    "                    word_role[self.translation(lemma_pred, src_lang, trg_lang)[0]] = 'Pred'\n",
    "                else:\n",
    "                    print('=====Pred: {}'.format(lemma_pred))\n",
    "                for arg in event.args:\n",
    "                    lemma_arg  = lemma[sent_num][arg.begin]\n",
    "                    if targetLanguage is not None:\n",
    "                        #print('Arg({}): {}'.format(arg.tag, self.translation(lemma_arg, src_lang, trg_lang)[0]))\n",
    "                        word_role[self.translation(lemma_arg, src_lang, trg_lang)[0]] = 'Arg({})'.format(arg.tag)\n",
    "                    else:\n",
    "                        print('Arg({}): {}'.format(arg.tag, lemma_arg))\n",
    "        return word_role\n",
    "    \n",
    "    def get_srl_turkic(self, rus_text, selected_language):\n",
    "        # texts = [\"Без дачага киттек.\", \"Балалар өйдән кача һәм урманда югалып кала.\",\"Әни рамны юды. Әти машинаны йөртте.\"]\n",
    "        # словарь - лемма:роль\n",
    "        # src_code = self.language_code_dict[selected_language]\n",
    "        # translation = translate(text, source_language=src_code )\n",
    "        # # print(translations)\n",
    "        # print(f'исходный  текст на {selected_language[:-2]}ом', end='\\n\\n')\n",
    "        # rus_text = translation\n",
    "        # print('переведенный текст на русском', end='\\n\\n')\n",
    "        # print(rus_text)\n",
    "        #print('get_srl_turkic_rus_text', rus_text)\n",
    "        res = ppl(rus_text)\n",
    "        # print(f'разметка семантических ролей на русском')\n",
    "        # print_roles(res['lemma'], res['srl'])\n",
    "        # print()\n",
    "        # print(f'разметка семантических ролей на {selected_language[:-2]}ом')\n",
    "        return self.print_roles(res['lemma'], res['srl'], selected_language)\n",
    "        \n",
    "        \n",
    "    def predict(self, input_sentences, language_ru_name):\n",
    "        \n",
    "        languages_rus__eng = {'татарский':'Tatar', 'казахский':'Kazakh', 'киргизский':'Kyrgyz', 'башкирский':'Bashqort'}\n",
    "        for input_sentence in input_sentences:\n",
    "            #перевод\n",
    "            rus_sentence = self.translation(input_sentence, source_language=self.language_code_dict[language_ru_name])[0]\n",
    "            # токенизация исходного предложения\n",
    "            #input_words = tokenize(input_sentence)\n",
    "            # стемминг слов исходного предложения\n",
    "            stem_input_words = self.get_morph(input_sentence, languages_rus__eng[language_ru_name])\n",
    "            #print('stems',stem_input_words)\n",
    "            # получить список словарей лемма - роль\n",
    "            word_roles = self.get_srl_turkic(rus_sentence, language_ru_name)\n",
    "            #print('word roles',word_roles)\n",
    "            # применить к леммам стемминг\n",
    "            stem_role = {self.get_morph(k, languages_rus__eng[language_ru_name])[0]: v for k, v in word_roles.items()}\n",
    "            #print('stem role', stem_role)\n",
    "            # разметить всю последовательность по словарям полученным выше\n",
    "            result = {}\n",
    "            for stem_input_word in stem_input_words:\n",
    "                if stem_input_word in stem_role:\n",
    "                    result[stem_input_word] = stem_role[stem_input_word]\n",
    "                else:\n",
    "                    result[stem_input_word] = 'O'\n",
    "            result_input = {}\n",
    "            for k, v in zip(self.tokenize(input_sentence), result.values()):\n",
    "                result_input[k] = v\n",
    "            return result_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed7fe3d8-aa5e-4b89-96ab-c5b9b1ce1ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "srl = Model_srl1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c017940a-d393-4ef7-bd84-ffa6bfd6c6ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'browser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:4\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mModel_srl1.predict\u001b[1;34m(self, input_sentences, language_ru_name)\u001b[0m\n\u001b[0;32m    134\u001b[0m rus_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslation(input_sentence, source_language\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlanguage_code_dict[language_ru_name])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# токенизация исходного предложения\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m#input_words = tokenize(input_sentence)\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# стемминг слов исходного предложения\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m stem_input_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_morph\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguages_rus__eng\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlanguage_ru_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m#print('stems',stem_input_words)\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# получить список словарей лемма - роль\u001b[39;00m\n\u001b[0;32m    141\u001b[0m word_roles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_srl_turkic(rus_sentence, language_ru_name)\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mModel_srl1.get_morph\u001b[1;34m(self, text, language)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_morph\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, language):\n\u001b[1;32m---> 55\u001b[0m     lang_field \u001b[38;5;241m=\u001b[39m \u001b[43mbrowser\u001b[49m\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/html/body/div/div[1]/div[1]/form/div[1]/div/select\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     56\u001b[0m     lang_field\u001b[38;5;241m.\u001b[39msend_keys(language)\n\u001b[0;32m     58\u001b[0m     text_field \u001b[38;5;241m=\u001b[39m browser\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mXPATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/html/body/div/div[1]/div[1]/form/div[2]/div/textarea\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mclear()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'browser' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open('text_tt.txt', encoding='utf-8') as f:\n",
    "    texts_tt = f.readlines()\n",
    "selected_language = 'татарский'\n",
    "result = srl.predict(texts_tt, selected_language)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f2a7ec-4027-4ca8-8cc0-a915110e76ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "made-py39-venv",
   "language": "python",
   "name": "made-py39-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
